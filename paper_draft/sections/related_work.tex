\section{Related Work}
\label{sec:related_work}

We organize related work into three threads: benchmarks for epistemic reasoning in LLMs, mechanistic studies of belief representation, and work on the stability and robustness of LLM belief-like behavior.

\para{Epistemic reasoning benchmarks.}
Several benchmarks test whether LLMs can reason about beliefs and knowledge states. The \kable benchmark~\citep{suzgun2024belief} evaluates 15 LLMs across 13,000 epistemic reasoning questions, finding that models achieve 85.7\% accuracy on factual belief confirmation but only 54.4\% on false belief tasks---revealing a systematic inability to represent beliefs that diverge from world knowledge. \citet{sileo2023mindgames} use dynamic epistemic logic to test formal epistemic reasoning, finding near-chance performance for most models and only 70\% accuracy for GPT-4. Additional ToM benchmarks including \hitom~\citep{he2023hitom}, \fantom~\citep{kim2023fantom}, and \simpletom~\citep{strachan2024simpletom} consistently show that LLM performance degrades with increasing epistemic complexity. \citet{gandhi2023howfar} provide a comprehensive assessment confirming that LLMs remain far from genuine belief-reasoning agents. Unlike these studies, which test whether LLMs can reason \emph{about} beliefs, we test whether LLMs treat different \emph{types} of beliefs differently in their own responses.

\para{Internal representations of belief.}
A growing body of work probes how LLMs internally encode belief states. \citet{bortoletto2024representations} discover separate linear representations for self-beliefs versus attributed beliefs of characters in Mistral-7B, with activation interventions causally altering belief attributions. \citet{lanham2025lookbacks} identify a ``lookback mechanism''---a pointer-dereference attention pattern in Llama-3-70B that implements belief tracking by attending to narrative points where beliefs were formed. \citet{bigelow2025belief} formalize LLM belief dynamics through a Bayesian framework, showing that in-context learning operates as evidence accumulation (epistemic updating) while activation steering operates as prior modification (non-epistemic disposition change), with these two mechanisms being additive in log-odds space. This dual-pathway finding is particularly relevant to our work: it suggests a mechanistic basis for the epistemic/non-epistemic distinction that we probe behaviorally. \citet{herrmann2024standards} propose four criteria (accuracy, coherence, uniformity, use) for genuine belief representations, arguing that current LLMs likely fail on uniformity. Our behavioral approach complements these representational studies by testing whether internal distinctions manifest in observable behavior.

\para{Stability and robustness of LLM beliefs.}
Several studies examine whether LLM belief-like behavior is anchored in epistemic content or driven by non-epistemic contextual features. \citet{krastev2025epistemic} find that prompt framing systematically modulates misinformation correction: creative intent reduces correction by 89\%, and expert roles reduce correction by 21\%. \citet{dies2026representational} introduce the \pstat framework, showing that epistemic familiarity governs belief stability---synthetic content destabilizes beliefs far more than fictional content---and that LLMs conflate distributional plausibility with epistemic justification. \citet{li2023beliefrevision} study how LLMs update beliefs under contradictory evidence, finding inconsistent revision patterns. \citet{kassner2021beliefbank} propose structured belief storage for systematic consistency. These findings paint a picture of LLMs whose epistemic behavior is fragile and context-dependent. Our work extends this line by asking whether the \emph{type} of belief (epistemic vs.\ non-epistemic) itself modulates LLM behavior, independent of contextual framing.
