Humans distinguish between epistemic beliefs---those grounded in evidence and aimed at tracking truth---and non-epistemic beliefs---those rooted in values, desires, or faith. This distinction is fundamental to epistemology, yet no prior work has tested whether large language models (LLMs) maintain an analogous behavioral differentiation. We present four experiments probing whether GPT-4.1 treats epistemic and non-epistemic beliefs differently across explicit classification, spontaneous response patterns, belief revision under counterevidence, and factive verb sensitivity. We find that GPT-4.1 exhibits strong differentiation: it classifies belief types with 100\% accuracy ($\kappa = 1.0$), spontaneously employs evidence-based reasoning for epistemic beliefs and values-based reasoning for non-epistemic beliefs (Cram\'{e}r's $V = 0.95$), and correctly handles the presuppositional semantics of factive verbs ($p < 0.0001$). Notably, the model is \emph{more} resistant to revising well-established factual beliefs than value-based beliefs (Cohen's $d = -0.45$, $p = 0.002$), the opposite of the naive philosophical prediction. These findings demonstrate that frontier LLMs are not ``epistemically flat''---they have internalized distinct epistemic norms for different belief types---and carry direct implications for AI safety and alignment.
