\section{Conclusion}
\label{sec:conclusion}

We present the first behavioral study directly testing whether LLMs differentiate epistemic beliefs from non-epistemic beliefs. Across four experiments, we find that \gptmodel maintains a robust functional distinction: it classifies belief types perfectly, spontaneously employs different reasoning strategies for each type, handles factive verb semantics correctly, and---most strikingly---resists revising well-established factual beliefs more than value-based beliefs. These findings demonstrate that frontier LLMs have internalized epistemic norms that mirror a fundamental distinction in human cognition, with direct implications for how we design, deploy, and align these systems.

\para{Future work.}
Three directions are immediate priorities. First, multi-model comparison across Claude, Gemini, and open-source models will establish the generalizability of our findings. Second, testing with ambiguous beliefs that straddle the epistemic/non-epistemic boundary will reveal the limits of the model's differentiation. Third, systematically varying counterevidence quality will better characterize the revision asymmetry and determine whether the model can distinguish valid from fabricated challenges to factual beliefs.
