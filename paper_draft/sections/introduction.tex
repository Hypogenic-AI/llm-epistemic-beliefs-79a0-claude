\section{Introduction}
\label{sec:introduction}

Not all beliefs are created equal. When a person says ``I believe the Earth orbits the Sun,'' they express something categorically different from ``I believe honesty is the most important virtue.'' The first is an \newterm{epistemic belief}---grounded in evidence, amenable to empirical verification, and governed by truth-tracking norms. The second is a \newterm{non-epistemic belief}---anchored in values, shaped by personal experience, and evaluated by standards other than correspondence with objective reality. This distinction is foundational to epistemology and central to how humans reason, argue, and revise their views~\citep{stalnaker1984inquiry}.

Given the rapid deployment of LLMs in settings that require navigating both factual claims and value judgments---from educational tutoring to medical advice to political discourse---a natural question arises: {\bf do LLMs behaviorally differentiate between epistemic and non-epistemic beliefs?} If they do, this has direct consequences for AI safety: models that treat factual misinformation and value disagreements identically may fail to challenge dangerous falsehoods or may inappropriately challenge legitimate moral convictions. If they do not, this reveals a fundamental gap in their capacity to engage appropriately with the diverse landscape of human belief.

Prior work has examined whether LLMs can reason \emph{about} beliefs---tracking who knows what in Theory of Mind (ToM) tasks~\citep{sap2023evaluating, sileo2023mindgames, kim2023fantom}---and has probed the epistemic robustness of LLM outputs~\citep{krastev2025epistemic, dies2026representational}. The \kable benchmark~\citep{suzgun2024belief} revealed that LLMs struggle to distinguish knowledge from mere belief, achieving only 54.4\% accuracy on false belief tasks. However, a critical gap remains: {\bf no study has directly tested whether LLMs treat different \emph{types} of beliefs differently in their behavioral responses.} Existing work asks whether models can reason about epistemic states, but not whether they deploy different reasoning strategies depending on whether a belief is epistemic or non-epistemic.

We address this gap with a behavioral probing study that tests whether \gptmodel differentiates epistemic from non-epistemic beliefs across four dimensions. We construct a novel stimulus set of 40 belief statements (20 epistemic, 20 non-epistemic) and probe the model's responses through: (1) explicit classification, (2) spontaneous response elicitation, (3) belief revision under counterevidence, and (4) factive verb sensitivity.

Our results reveal that \gptmodel maintains a robust functional distinction between belief types. The model classifies beliefs with 100\% accuracy, spontaneously employs evidence-based reasoning for epistemic beliefs and values-based reasoning for non-epistemic beliefs (Cram\'{e}r's $V = 0.95$), and correctly handles factive verb semantics ($p < 0.0001$). Most strikingly, the model is \emph{more resistant} to revising well-established factual beliefs than value-based beliefs (Cohen's $d = -0.45$, $p = 0.002$), reversing the naive philosophical prediction. This asymmetry suggests the model has learned to weight strong prior evidence for established facts while treating value claims as inherently contestable.

In summary, our main contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item We design the first behavioral probing study that directly tests whether LLMs differentiate epistemic from non-epistemic beliefs, addressing a gap in the literature on LLM epistemic reasoning.
    \item We construct a novel stimulus set of 40 belief statements spanning scientific facts, historical facts, mathematical truths, moral values, life philosophy, and faith-based claims, paired with matched counterevidence for revision experiments.
    \item We conduct four experiments revealing that \gptmodel maintains a strong functional distinction between belief types, with large effect sizes across all dimensions (Cram\'{e}r's $V$ up to 0.95, Cohen's $d = 0.45$).
    \item We identify a counterintuitive revision asymmetry---the model resists revising epistemic beliefs more than non-epistemic ones---and analyze its implications for AI safety and alignment.
\end{itemize}
