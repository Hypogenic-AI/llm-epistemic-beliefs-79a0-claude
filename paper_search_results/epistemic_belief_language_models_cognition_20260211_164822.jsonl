{"title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind", "year": 2024, "authors": "Lance Ying, Tan Zhi-Xuan, Lionel Wong, Vikash K. Mansinghka, Joshua B. Tenenbaum", "url": "https://api.semanticscholar.org/CorpusId:271923781", "relevance": 3, "abstract": "\n How do people understand and evaluate claims about others\u2019 beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents\u2019 goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic \u201clanguage-of-thought\u201d with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent\u2019s beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.", "citations": 5}
{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "year": 2024, "authors": "Mirac Suzgun, Tayfun Gur, Federico Bianchi, Daniel E. Ho, Thomas Icard, Dan Jurafsky, James Zou", "url": "https://api.semanticscholar.org/CorpusId:273655169", "relevance": 3, "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to significant consequences in areas such as medical diagnosis, legal judgments, and dissemination of fake news. Despite this, current literature has largely focused on more complex issues such as theory of mind, overlooking more fundamental epistemic challenges. This study systematically evaluates the epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13 tasks. Our results reveal key limitations. First, while LMs achieve 86% accuracy on factual scenarios, their performance drops significantly with false scenarios, particularly in belief-related tasks. Second, LMs struggle with recognizing and affirming personal beliefs, especially when those beliefs contradict factual data, which raises concerns for applications in healthcare and counseling, where engaging with a person's beliefs is critical. Third, we identify a salient bias in how LMs process first-person versus third-person beliefs, performing better on third-person tasks (80.7%) compared to first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the factive nature of knowledge, namely, that knowledge inherently requires truth. Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the deeper reasoning. These findings highlight significant concerns about current LMs' ability to reason about truth, belief, and knowledge while emphasizing the need for advancements in these areas before broad deployment in critical sectors.", "citations": 7}
{"title": "The Development of Epistemological Theories: Beliefs About Knowledge and Knowing and Their Relation to Learning", "year": 1997, "authors": "B. Hofer, P. Pintrich", "url": "https://www.semanticscholar.org/paper/55677a9eb759dac6220d88cee2f1718f02a181be", "relevance": 3, "abstract": "", "citations": 3217}
{"title": "Grounding Language about Belief in a Bayesian Theory-of-Mind", "year": 2024, "authors": "Lance Ying, Tan Zhi-Xuan, Lionel Wong, Vikash K. Mansinghka, Joshua B. Tenenbaum", "url": "https://api.semanticscholar.org/CorpusId:267740428", "relevance": 3, "abstract": "Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others' beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve a doors-and-keys gridworld puzzle that requires instrumental reasoning about hidden objects. In contrast to pure logical deduction, non-mentalizing baselines, and mentalizing that ignores the role of instrumental plans, our model provides a much better fit to human goal and belief attributions, demonstrating the importance of theory-of-mind for a semantics of belief.", "citations": 9}
{"title": "Modeling Epistemic and Ontological Cognition: Philosophical Perspectives and Methodological Directions", "year": 2008, "authors": "J. A. Greene, R. Azevedo, J. Torney-Purta", "url": "https://www.semanticscholar.org/paper/072c1ca4da4ed603a1eda196f43044c6b769aa44", "relevance": 3, "abstract": "", "citations": 264}
{"title": "From Evidence to Belief: A Bayesian Epistemology Approach to Language Models", "year": 2025, "authors": "Minsu Kim, Sangryul Kim, James Thorne", "url": "https://api.semanticscholar.org/CorpusId:278164768", "relevance": 3, "abstract": "This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models' responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not consistently follow Bayesian epistemology: language models follow the Bayesian confirmation assumption well with true evidence but fail to adhere to other Bayesian assumptions when encountering different evidence types. Also, we demonstrated that language models can exhibit high confidence when given strong evidence, but this does not always guarantee high accuracy. Our analysis also reveals that language models are biased toward golden evidence and show varying performance depending on the degree of irrelevance, helping explain why they deviate from Bayesian assumptions.", "citations": 0}
{"title": "Epistemic Beliefs in Science\u2014A Systematic Integration of Evidence From Multiple Studies", "year": 2022, "authors": "Julia Schiefer, P. Edelsbrunner, Andrea Bernholt, Nele Kampa, A. Nehring", "url": "https://www.semanticscholar.org/paper/6674eef3a59f99c7b71d19f76e3e7fbd11aaa5f9", "relevance": 3, "abstract": "Recent research has integrated developmental and dimensional perspectives on epistemic beliefs by implementing an approach in which profiles of learners\u2019 epistemic beliefs are modeled across multiple dimensions. Variability in study characteristics has impeded the comparison of profiles of epistemic beliefs and their relations with external variables across studies. We examined this comparability by integrating data on epistemic beliefs about the source, certainty, development, and justification of knowledge in science from six studies comprising N\u2009=\u200910,932 German students from elementary to upper secondary school. Applying latent profile analyses to these data, we found that profiles of epistemic beliefs that were previously conceptualized were robust across multiple samples. We found indications that profiles of epistemic beliefs homogenize over the course of students\u2019 education, are related to school tracking, and demonstrate robust relations with students\u2019 personal characteristics and socioeconomic background. We discuss implications for the theory, assessment, and education of epistemic beliefs.", "citations": 36}
{"title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models", "year": 2024, "authors": "Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, Yingchun Wang", "url": "https://api.semanticscholar.org/CorpusId:273507547", "relevance": 3, "abstract": "With large language models (LLMs) increasingly deployed as cognitive engines for AI agents, the reliability and effectiveness critically hinge on their intrinsic epistemic agency, which remains understudied. Epistemic agency, the ability to flexibly construct, adapt, and monitor beliefs about dynamic environments, represents a base-model-level capacity independent of specific tools, modules, or applications. We characterize the holistic process underlying epistemic agency, which unfolds in seven interrelated dimensions: prediction, decision-making, perception, memory, counterfactual thinking, belief updating, and meta-reflection. Correspondingly, we propose Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven tasks with long-term relevance and minimization of data leakage. Through a comprehensive evaluation of 16 models using three prompting strategies, we identify a clear three-tier performance hierarchy and significant limitations of current LLMs, particularly in meta-reflection capabilities. While state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our findings suggest several promising research directions, including enhancing core cognitive functions, improving cross-functional coordination, and developing adaptive processing mechanisms. Our code and data are available at https://github.com/AI45Lab/ReflectionBench.", "citations": 2}
{"title": "University students\u2019 epistemic profiles, conceptions of learning, and academic performance", "year": 2020, "authors": "K. Lonka, Elina E. Ketonen, J. Vermunt", "url": "https://www.semanticscholar.org/paper/e0ca92176218cc76c909a9723f1cd1aa940f9ec9", "relevance": 3, "abstract": "University students\u2019 epistemic beliefs may have practical consequences for studying and success in higher education. Such beliefs constitute epistemic theories that may empirically manifest themselves as epistemic profiles. This study examined university students\u2019 epistemic profiles and their relations to conceptions of learning, age, gender, discipline, and academic achievement. The participants were 1515 students from five faculties who completed questionnaires about epistemic beliefs, including a subsample who also completed a questionnaire that included conceptions of learning. We measured epistemic beliefs: reflective learning, collaborative knowledge-building, valuing metacognition, certain knowledge, and practical value. First, we analyzed structural validity by using confirmatory factor analysis. Second, we conducted latent profile analysis that revealed three epistemic profiles: Pragmatic (49%), reflective-collaborative (26%) and fact-oriented (25%). Then, we compared the conceptions of learning across the profiles as well as demographic information, credits, and grades. The profiles\u2019 conceptions of learning varied: The reflective-collaborative group scored high on conception of learning named \u201cconstruction of knowledge.\u201d Its members were more likely to be females, teachers, and mature students, and they had the highest academic achievement. The fact-oriented group (mostly engineering/science students) scored highest on \u201cintake of knowledge.\u201d The pragmatic group scored highest on \u201cuse of knowledge:\u201d During the second year, their academic achievement improved. In sum, the epistemic profiles were closely related to conceptions of learning and also associated with academic achievement.", "citations": 53}
{"title": "The Art of Saying No: Contextual Noncompliance in Language Models", "year": 2024, "authors": "Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, K. Chandu, Jack Hessel, Yulia Tsvetkov, Noah A. Smith, Yejin Choi, Hanna Hajishirzi", "url": "https://api.semanticscholar.org/CorpusId:271244871", "relevance": 3, "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\"unsafe\"queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.", "citations": 66}
{"title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "year": 2025, "authors": "Lance Ying, Almog Hillel, Ryan Truong, Vikash K. Mansinghka, Joshua B. Tenenbaum, Tan Zhi-Xuan", "url": "https://www.semanticscholar.org/paper/d8ed7dbac692e14892cd28feb9654cfb86d1584c", "relevance": 3, "abstract": "A key feature of human theory-of-mind is the ability to attribute beliefs to other agents as mentalistic explanations for their behavior. But given the wide variety of beliefs that agents may hold about the world and the rich language we can use to express them, which specific beliefs are people inclined to attribute to others? In this paper, we investigate the hypothesis that people prefer to attribute beliefs that are good explanations for the behavior they observe. We develop a computational model that quantifies the explanatory strength of a (natural language) statement about an agent's beliefs via three factors: accuracy, informativity, and causal relevance to actions, each of which can be computed from a probabilistic generative model of belief-driven behavior. Using this model, we study the role of each factor in how people selectively attribute beliefs to other agents. We investigate this via an experiment where participants watch an agent collect keys hidden in boxes in order to reach a goal, then rank a set of statements describing the agent's beliefs about the boxes' contents. We find that accuracy and informativity perform reasonably well at predicting these rankings when combined, but that causal relevance is the single factor that best explains participants' responses.", "citations": 0}
{"title": "Expanding the Dimensions of Epistemic Cognition: Arguments From Philosophy and Psychology", "year": 2011, "authors": "C. Chinn, Luke A. Buckland, A. Samarapungavan", "url": "https://www.semanticscholar.org/paper/cbfd22c3761bf5267bc937d5a0877450fb26cdbf", "relevance": 3, "abstract": "", "citations": 455}
{"title": "Parameterized Complexity of Theory of Mind Reasoning in Dynamic Epistemic Logic", "year": 2018, "authors": "Iris van de Pol, I. Rooij, Jakub Szymanik", "url": "https://api.semanticscholar.org/CorpusId:52002617", "relevance": 3, "abstract": "Theory of mind refers to the human capacity for reasoning about others\u2019 mental states based on observations of their actions and unfolding events. This type of reasoning is notorious in the cognitive science literature for its presumed computational intractability. A possible reason could be that it may involve higher-order thinking (e.g., \u2018you believe that I believe that you believe\u2019). To investigate this we formalize theory of mind reasoning as updating of beliefs about beliefs using dynamic epistemic logic, as this formalism allows to parameterize \u2018order of thinking.\u2019 We prove that theory of mind reasoning, so formalized, indeed is intractable (specifically, PSPACE-complete). Using parameterized complexity we prove, however, that the \u2018order parameter\u2019 is not a source of intractability. We furthermore consider a set of alternative parameters and investigate which of them are sources of intractability. We discuss the implications of these results for the understanding of theory of mind.", "citations": 38}
{"title": "Theoretical Foundations for Semantic Cognition in Artificial Intelligence", "year": 2025, "authors": "Sebastian Dumbrava", "url": "https://api.semanticscholar.org/CorpusId:278207661", "relevance": 3, "abstract": "This monograph presents a modular cognitive architecture for artificial intelligence grounded in the formal modeling of belief as structured semantic state. Belief states are defined as dynamic ensembles of linguistic expressions embedded within a navigable manifold, where operators enable assimilation, abstraction, nullification, memory, and introspection. Drawing from philosophy, cognitive science, and neuroscience, we develop a layered framework that enables self-regulating epistemic agents capable of reflective, goal-directed thought. At the core of this framework is the epistemic vacuum: a class of semantically inert cognitive states that serves as the conceptual origin of belief space. From this foundation, the Null Tower arises as a generative structure recursively built through internal representational capacities. The theoretical constructs are designed to be implementable in both symbolic and neural systems, including large language models, hybrid agents, and adaptive memory architectures. This work offers a foundational substrate for constructing agents that reason, remember, and regulate their beliefs in structured, interpretable ways.", "citations": 3}
{"title": "Epistemic Integrity in Large Language Models", "year": 2024, "authors": "Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-Junqi Tian, Mayank Goel, Reihaneh Rabbany, J. Godbout, Kellin Pelrine", "url": "https://api.semanticscholar.org/CorpusId:273963769", "relevance": 3, "abstract": "Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.", "citations": 4}
{"title": "Modeling relationships among beliefs about scientific knowledge, beliefs about justification for knowing in science, and argumentative reasoning about a climate-related issue", "year": 2024, "authors": "Fang-Ying Yang, Kaushal Kumar Bhagat, Christian Brandmo, Wan-Yue Zhan, Ivar Br\u00e5ten", "url": "https://api.semanticscholar.org/CorpusId:274682847", "relevance": 3, "abstract": "In a sample of 323 engineering students, structural equation modeling was used to test hypothesized relationships between beliefs about the nature of knowledge in science, beliefs about the process of justification for knowing in science, and argumentative reasoning about an ill-structured social-scientific issue. Beliefs about justification for knowing were directly related to argumentative reasoning, with beliefs in justification by school-based authority being a negative predictor and beliefs in justification by research-based authority being a positive predictor of students\u2019 evidence- and knowledge-based reasoning, and with beliefs in personal justification being a negative predictor of students\u2019 consideration of counterarguments. These beliefs about justification for knowing in science were, in turn, predicted by students\u2019 beliefs about the certainty and development of scientific knowledge. The discussion highlights the multileveled and multidimensional view of epistemic beliefs that follows from our investigation, as well as the role of such beliefs in students\u2019 argumentative reasoning.", "citations": 1}
{"title": "Reducing Conversational Agents\u2019 Overconfidence Through Linguistic Calibration", "year": 2020, "authors": "Sabrina J. Mielke, Arthur Szlam, Emily Dinan, Y-Lan Boureau", "url": "https://www.semanticscholar.org/paper/d77c78c9439422ed88e754f776a642d43a8acb66", "relevance": 3, "abstract": "Abstract While improving neural dialogue agents\u2019 factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model\u2019s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.", "citations": 224}
{"title": "Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents", "year": 2023, "authors": "Gregor Betz, Kyle Richardson", "url": "https://api.semanticscholar.org/CorpusId:256696059", "relevance": 3, "abstract": "It is argued that suitably trained neural language models exhibit key properties of epistemic agency: they hold probabilistically coherent and logically consistent degrees of belief, which they can rationally revise in the face of novel evidence. To this purpose, we conduct computational experiments with rankers: T5 models [Raffel et al. 2020] that are pretrained on carefully designed synthetic corpora. Moreover, we introduce a procedure for eliciting a model\u2019s degrees of belief, and define numerical metrics that measure the extent to which given degrees of belief violate (probabilistic, logical, and Bayesian) rationality constraints. While pretrained rankers are found to suffer from global inconsistency (in agreement with, e.g., [Jang et al. 2021]), we observe that subsequent self-training on auto-generated texts allows rankers to gradually obtain a probabilistically coherent belief system that is aligned with logical constraints. In addition, such self-training is found to have a pivotal role in rational evidential learning, too, for it seems to enable rankers to propagate a novel evidence item through their belief systems, successively re-adjusting individual degrees of belief. All this, we conclude, confirms the Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills. We suggest that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve.", "citations": 2}
{"title": "Epistemic Emotions and Epistemic Cognition Predict Critical Thinking About Socio-Scientific Issues", "year": 2021, "authors": "Krista R. Muis, Marianne Chevrier, Courtney Denton, Kelsey M. Losenno", "url": "https://api.semanticscholar.org/CorpusId:233228812", "relevance": 3, "abstract": "When thinking critically about socio-scientific issues, individuals\u2019 expectations about the nature of knowledge and knowing, as well as their emotions when these expectations are met or not, may play an important role in critical thinking. In this study, we examined the role of epistemic emotions in mediating the effects of epistemic cognition on critical thinking when contending with conflicting information about genetically modified foods. Two hundred four university students completed a prior knowledge test on genetically modified foods, and then reported their epistemic beliefs about genetically modified foods. Participants then read a text that presented advantages and disadvantages of genetically modified foods, and reported the epistemic emotions they experienced during reading of that text. Participants then composed an argumentative essay about genetically modified foods, which were coded for critical thinking. Results from path analysis revealed that a belief in complex knowledge predicted less surprise and confusion, but more enjoyment. For the source of knowledge, a belief in the active construction of knowledge predicted less surprise and enjoyment. For justification for knowing, a belief that knowledge should be critically evaluated positively predicted curiosity, and negatively predicted confusion and boredom. Moreover, beliefs that knowledge about genetically modified foods is complex and uncertain positively predicted critical thinking. Confusion and anxiety also positively predicted critical thinking, whereas frustration negatively predicted critical thinking. Lastly, confusion mediated relations between epistemic beliefs and critical thinking. Results suggest complex relations between epistemic cognition, epistemic emotions, and critical thinking that have implications for educational practice as well as for future research on epistemic cognition and epistemic emotions.", "citations": 45}
{"title": "Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence", "year": 2025, "authors": "Saleh Nikooroo", "url": "https://api.semanticscholar.org/CorpusId:280526926", "relevance": 3, "abstract": "Belief systems are often treated as globally consistent sets of propositions or as scalar-valued probability distributions. Such representations tend to obscure the internal structure of belief, conflate external credibility with internal coherence, and preclude the modeling of fragmented or contradictory epistemic states. This paper introduces a minimal formalism for belief systems as directed, weighted graphs. In this framework, nodes represent individual beliefs, edges encode epistemic relationships (e.g., support or contradiction), and two distinct functions assign each belief a credibility (reflecting source trust) and a confidence (derived from internal structural support). Unlike classical probabilistic models, our approach does not assume prior coherence or require belief updating. Unlike logical and argumentation-based frameworks, it supports fine-grained structural representation without committing to binary justification status or deductive closure. The model is purely static and deliberately excludes inference or revision procedures. Its aim is to provide a foundational substrate for analyzing the internal organization of belief systems, including coherence conditions, epistemic tensions, and representational limits. By distinguishing belief structure from belief strength, this formalism enables a richer classification of epistemic states than existing probabilistic, logical, or argumentation-based approaches.", "citations": 1}
{"title": "Epistemic Fragility in Large Language Models: Prompt Framing Systematically Modulates Misinformation Correction", "year": 2025, "authors": "S. Krastev, Hilary Sweatman, Anni Sternisko, Steve Rathje", "url": "https://api.semanticscholar.org/CorpusId:283438279", "relevance": 3, "abstract": "As large language models (LLMs) rapidly displace traditional expertise, their capacity to correct misinformation has become a core concern. We investigate the idea that prompt framing systematically modulates misinformation correction - something we term'epistemic fragility'. We manipulated prompts by open-mindedness, user intent, user role, and complexity. Across ten misinformation domains, we generated 320 prompts and elicited 2,560 responses from four frontier LLMs, which were coded for strength of misinformation correction and rectification strategy use. Analyses showed that creative intent, expert role, and closed framing led to a significant reduction in correction likelihood and effectiveness of used strategy. We also found striking model differences: Gemini 2.5 Pro had 74% lower odds of strong correction than Claude Sonnet 4.5. These findings highlight epistemic fragility as an important structural property of LLMs, challenging current guardrails and underscoring the need for alignment strategies that prioritize epistemic integrity over conversational compliance.", "citations": 1}
{"title": "Critical Thinking and Epistemic Sophistication in Science Education", "year": 2025, "authors": "O. E. Tamayo Alzate", "url": "https://www.semanticscholar.org/paper/7a41bf63ecd18d90ba62655c0dd0f15cd76c578c", "relevance": 3, "abstract": "One of the central purposes of education at different educational levels is to contribute to the formation of critical thinking in students. There are many theoretical perspectives from which critical thinking is conceptualized, such as those centers on the development of students\u2019 capacities and those based on competences, skills, dispositions and criteria, among others. We consider that in the school context the critical thinking perspective that should come first is the domain-specific one; consequently, we present a conceptual model for the formation of critical thinking in the context of science teaching and learning in the classroom constituted by the integration of four dimensions: languages and argumentation, metacognition, emotions, and problem solving and decision making. Our focus of reflection is epistemic cognition with the processes of epistemic sophistication, metacognitive sophistication, and metaemotional sophistication, determinants of critical thinking in relation to each of the dimensions and the relationships between them. We conclude with the proposal of a conceptual model for the development of critical thinking based on students\u2019 epistemic cognition.", "citations": 3}
{"title": "University students\u2019 beliefs about science and their relationship with knowledge about science", "year": 2023, "authors": "Cornelia Schoor", "url": "https://api.semanticscholar.org/CorpusId:259620221", "relevance": 3, "abstract": "Science and personal experiences in some cases seem to be two different ways of knowledge justification. The current \u201cpost-truth\u201d era is characterized by a rise of personal beliefs and justifications. In order to address these phenomena from a perspective of beliefs, several constructs may be considered: Beliefs about the utility of science and of personal experiences, trust in science, and epistemic beliefs. Despite some research addressing each belief\u2019s independent relation to information seeking behavior, we do not know much about the interrelationship of these beliefs. To address this research gap and to explore whether knowledge about how science works is related to these beliefs, a paper\u2013pencil study with 315 university students of psychology, education, and teacher education was conducted. There was a high positive relationship of trust in science with justification-by-authority beliefs, and medium negative relationships of trust in science with uncertainty beliefs and personal-justification beliefs. Trust in science was positively related to the perceived utility of science. Epistemic beliefs were also related to utility beliefs. The number of methods courses taken and knowledge about how science works was related to trust in science and epistemic beliefs, but not to utility of science or utility of personal experiences. It is concluded that we should revisit our conceptualization of epistemic beliefs in the context of \u201cpost-truth\u201d.", "citations": 4}
{"title": "Socialising Epistemic Cognition", "year": 2017, "authors": "Simon Knight, K. Littleton", "url": "https://api.semanticscholar.org/CorpusId:152018496", "relevance": 3, "abstract": "", "citations": 11}
{"title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes", "year": 2025, "authors": "Meng Li, Michael Vrazitulis, David Schlangen", "url": "https://api.semanticscholar.org/CorpusId:279119784", "relevance": 3, "abstract": "Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.", "citations": 0}
{"title": "Parameterized Complexity Results for a Model of Theory of Mind Based on Dynamic Epistemic Logic", "year": 2016, "authors": "Iris van de Pol, I. Rooij, Jakub Szymanik", "url": "https://www.semanticscholar.org/paper/4b16adb4418cafe66d07902d932da83891a6df9b", "relevance": 3, "abstract": "In this paper we introduce a computational-level model of theory of mind (ToM) based on dynamic epistemic logic (DEL), and we analyze its computational complexity. The model is a special case of DEL model checking. We provide a parameterized complexity analysis, considering several aspects of DEL (e.g., number of agents, size of preconditions, etc.) as parameters. We show that model checking for DEL is PSPACE-hard, also when restricted to single-pointed models and S5 relations, thereby solving an open problem in the literature. Our approach is aimed at formalizing current intractability claims in the cognitive science literature regarding computational models of ToM.", "citations": 8}
{"title": "Representational and Behavioral Stability of Truth in Large Language Models", "year": 2025, "authors": "Samantha Dies, Courtney Maynard, Germans Savcisens, Tina Eliassi-Rad", "url": "https://api.semanticscholar.org/CorpusId:283244203", "relevance": 3, "abstract": "Large language models (LLMs) are increasingly used as information sources, yet small changes in semantic framing can destabilize their truth judgments. We propose P-StaT (Perturbation Stability of Truth), an evaluation framework for testing belief stability under controlled semantic perturbations in representational and behavioral settings via probing and zero-shot prompting. Across sixteen open-source LLMs and three domains, we compare perturbations involving epistemically familiar Neither statements drawn from well-known fictional contexts (Fictional) to those involving unfamiliar Neither statements not seen in training data (Synthetic). We find a consistent stability hierarchy: Synthetic content aligns closely with factual representations and induces the largest retractions of previously held beliefs, producing up to $32.7\\%$ retractions in representational evaluations and up to $36.3\\%$ in behavioral evaluations. By contrast, Fictional content is more representationally distinct and comparatively stable. Together, these results suggest that epistemic familiarity is a robust signal across instantiations of belief stability under semantic reframing, complementing accuracy-based factuality evaluation with a notion of epistemic robustness.", "citations": 0}
{"title": "Dynamic Epistemic Friction in Dialogue", "year": 2025, "authors": "Timothy Obiso, Kenneth Lai, Abhijnan Nath, Nikhil Krishnaswamy, James Pustejovsky", "url": "https://api.semanticscholar.org/CorpusId:279318387", "relevance": 3, "abstract": "Recent developments in aligning Large Language Models (LLMs) with human preferences have significantly enhanced their utility in human-AI collaborative scenarios. However, such approaches often neglect the critical role of\"epistemic friction,\"or the inherent resistance encountered when updating beliefs in response to new, conflicting, or ambiguous information. In this paper, we define dynamic epistemic friction as the resistance to epistemic integration, characterized by the misalignment between an agent's current belief state and new propositions supported by external evidence. We position this within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit, 2011), where friction emerges as nontrivial belief-revision during the interaction. We then present analyses from a situated collaborative task that demonstrate how this model of epistemic friction can effectively predict belief updates in dialogues, and we subsequently discuss how the model of belief alignment as a measure of epistemic resistance or friction can naturally be made more sophisticated to accommodate the complexities of real-world dialogue scenarios.", "citations": 2}
{"title": "Situation Theory and Channel theory as a Unified Framework for Imperfect Information Management", "year": 2022, "authors": "Farhad Naderian", "url": "https://api.semanticscholar.org/CorpusId:249394924", "relevance": 3, "abstract": ": This article argues that the Situation theory and the Channel theory can be used as a general framework for Imperfect Information Management. There are different kinds of imperfections in information like uncertainty, imprecision, vagueness, incompleteness, inconsistency, and context-dependency that can be handled pretty well by our brain as a cognitive agent. Basic approaches like probability theory and standard logic have just an ontological point of view. So these models are intrinsically inefficient in modeling fallacious minds. The generalization of these theories to the generalized probability and nonstandard logic theories has epistemological motivations to provide better models for information integration in cognitive agents. Among many models of them, possibility theory and probabilistic logic theory are the best approaches. Most of these models are based on possible world semantics. So I argue, based on a review of different approaches to Imperfect Information Management, that a good framework for it is the Situation theory of J. Barwise and the Channel theory of J. Barwise and J. Seligman. Also, I think that Situation theory has a better epistemological foundation to refer partiality as one of the main sources of imperfection in information. There are many features in these theories that can be used to model and represent different kinds of information both symbolic and numeric. They have relied on a powerful and unique notion of information. So they can encompass different kinds of information with different imperfections. These frameworks have a proper approach for context modeling to handle common knowledge and incomplete information. Also, they distinguish belief from knowledge clearly to model the non-monotonic and dynamic nature of knowledge. They discern the logic of the world from information flow in the mind. The objectification process in these theories reveals to us the nature of default or probabilistic rules in perceptions. The concept of the channel can be used to represent those types of reasoning mechanisms that move from one model or logic to another one. The imprecision in our perceptions causes fuzziness in reasoning and vagueness in communication that can be represented by some suitable classifications connected by some channels . This new framework like a network framework can provide a scalable and open framework to cover different models based on a relativistic notion of truth.", "citations": 0}
{"title": "The AI Cognitive Trojan Horse: How Large Language Models May Bypass Human Epistemic Vigilance", "year": 2026, "authors": "Andrew D. Maynard", "url": "https://api.semanticscholar.org/CorpusId:284648183", "relevance": 3, "abstract": "Large language model (LLM)-based conversational AI systems present a challenge to human cognition that current frameworks for understanding misinformation and persuasion do not adequately address. This paper proposes that a significant epistemic risk from conversational AI may lie not in inaccuracy or intentional deception, but in something more fundamental: these systems may be configured, through optimization processes that make them useful, to present characteristics that bypass the cognitive mechanisms humans evolved to evaluate incoming information. The Cognitive Trojan Horse hypothesis draws on Sperber and colleagues'theory of epistemic vigilance -- the parallel cognitive process monitoring communicated information for reasons to doubt -- and proposes that LLM-based systems present'honest non-signals': genuine characteristics (fluency, helpfulness, apparent disinterest) that fail to carry the information equivalent human characteristics would carry, because in humans these are costly to produce while in LLMs they are computationally trivial. Four mechanisms of potential bypass are identified: processing fluency decoupled from understanding, trust-competence presentation without corresponding stakes, cognitive offloading that delegates evaluation itself to the AI, and optimization dynamics that systematically produce sycophancy. The framework generates testable predictions, including a counterintuitive speculation that cognitively sophisticated users may be more vulnerable to AI-mediated epistemic influence. This reframes AI safety as partly a problem of calibration -- aligning human evaluative responses with the actual epistemic status of AI-generated content -- rather than solely a problem of preventing deception.", "citations": 0}
{"title": "Human-like Social Compliance in Large Language Models: Unifying Sycophancy and Conformity through Signal Competition Dynamics", "year": 2025, "authors": "Long Zhang, Wei-neng Chen", "url": "https://api.semanticscholar.org/CorpusId:284911400", "relevance": 3, "abstract": "The increasing integration of Large Language Models (LLMs) into decision-making frameworks has exposed significant vulnerabilities to social compliance, specifically sycophancy and conformity. However, a critical research gap exists regarding the fundamental mechanisms that enable external social cues to systematically override a model's internal parametric knowledge. This study introduces the Signal Competition Mechanism, a unified framework validated by assessing behavioral correlations across 15 LLMs and performing latent-space probing on three representative open-source models. The analysis demonstrates that sycophancy and conformity originate from a convergent geometric manifold, hereafter termed the compliance subspace, which is characterized by high directional similarity in internal representations. Furthermore, the transition to compliance is shown to be a deterministic process governed by a linear boundary, where the Social Emotional Signal effectively suppresses the Information Calibration Signal. Crucially, we identify a\"Transparency-Truth Gap,\"revealing that while internal confidence provides an inertial barrier, it remains permeable and insufficient to guarantee immunity against intense social pressure. By formalizing the Integrated Epistemic Alignment Framework, this research provides a blueprint for transitioning from instructional adherence to robust epistemic integrity.", "citations": 0}
{"title": "Students working with multiple conflicting documents on a scientific issue: relations between epistemic cognition while reading and sourcing and argumentation in essays.", "year": 2014, "authors": "Ivar Br\u00e5ten, Leila E. Ferguson, Helge I. Str\u00f8ms\u00f8, \u00d8. Anmarkrud", "url": "https://www.semanticscholar.org/paper/aa71a620c3f18af3966ead6b527deee0a5ca8d43", "relevance": 3, "abstract": "", "citations": 130}
{"title": "Standards for Belief Representations in LLMs", "year": 2024, "authors": "Daniel A. Herrmann, B. A. Levinstein", "url": "https://api.semanticscholar.org/CorpusId:270199407", "relevance": 3, "abstract": "As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.", "citations": 26}
{"title": "Educational self-regulation competence: toward a lifespan-based concept and assessment strategy", "year": 2021, "authors": "J. Bittner, Christian Stamov Ro\u00dfnagel, U. Staudinger", "url": "https://api.semanticscholar.org/CorpusId:239712663", "relevance": 3, "abstract": "Self-regulation is crucial for learning and achievement in educational and occupational contexts. Educational self-regulation has been conceptualized as a domain-specific, context-bound competence that is open to interventions. Beyond students\u2019 educational self-regulation (ESR), few studies have examined ESR across the lifespan as a basis of competence assessments. We contribute to adult ESR by discussing whether ESR competence applies to intermediate and higher self-regulation levels, as represented by workplace learning and career management. Furthermore, we discuss the interplay of epistemic beliefs and metacomprehension as core processes of ESR. Finally, we outline cornerstones of an assessment strategy for adult ESR.", "citations": 9}
{"title": "Disentangling the process of epistemic change: The role of epistemic volition.", "year": 2020, "authors": "Martin Kerwer, Tom Rosman, Oliver Wedderhoff, Anita Chasiotis", "url": "https://www.semanticscholar.org/paper/d4230840690b3ea3675ae845639800cffa628db8", "relevance": 3, "abstract": "BACKGROUND\nMany interventions on epistemic beliefs (i.e., individual beliefs about knowledge and knowing) are based on Bendixen and Rule's Integrative Model for Personal Epistemology Development. Empirically, however, the model is still insufficiently validated. This is especially true for its epistemic volition component - a will or desire to actively change one's beliefs.\n\n\nAIMS\nTo experimentally scrutinize the role of epistemic volition, we investigated (incremental) effects on epistemic change of an epistemic volition intervention.\n\n\nSAMPLE\n412 psychology students enrolled at German universities completed the study.\n\n\nMETHODS\nWe employed a randomized pre-post design with three experimental groups that differed in the administered epistemic volition and resolvable controversies interventions. The purpose of the latter was to initiate an epistemic change process, thereby laying the foundation for the epistemic volition intervention. Both data collection and interventions were conducted online. In addition to self-report measures, we applied a complementary source evaluation task to analyse epistemic change.\n\n\nRESULTS\nEven though we found small- to medium-sized changes in epistemic beliefs, these changes did not differ between experimental conditions. Exploratory analyses suggested, however, that source evaluation task performance might have been promoted by the epistemic volition intervention and that - across experimental groups - manipulation check measures on both interventions interacted positively.\n\n\nCONCLUSION\nUltimately, we failed to separate the effects that our epistemic volition intervention had on epistemic change from these of the resolvable controversies intervention. Nonetheless, our study makes some strong contributions to - and interconnects - the growing bodies of research on epistemic change and multiple source use.", "citations": 7}
{"title": "A semantic embedding space based on large language models for modelling human beliefs", "year": 2024, "authors": "Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An", "url": "https://api.semanticscholar.org/CorpusId:271865570", "relevance": 3, "abstract": "Beliefs form the foundation of human cognition and decision-making, guiding our actions and social connections. A model encapsulating beliefs and their interrelationships is crucial for understanding their influence on our actions. However, research on belief interplay has often been limited to beliefs related to specific issues and has relied heavily on surveys. Here we propose a method to study the nuanced interplay between thousands of beliefs by leveraging online user debate data and mapping beliefs onto a neural embedding space constructed using a fine-tuned large language model. This belief space captures the interconnectedness and polarization of diverse beliefs across social issues. Our findings show that positions within this belief space predict new beliefs of individuals and estimate cognitive dissonance on the basis of the distance between existing and new beliefs. This study demonstrates how large language models, combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human belief formation. Lee et al. fine-tune large language models on debate data to create belief embeddings that capture the nuanced relationships between a wide range of beliefs, thus offering insight into how people form new beliefs.", "citations": 8}
{"title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning", "year": 2025, "authors": "Zhonghao He, Tianyi Alex Qiu, Hirokazu Shirado, Maarten Sap", "url": "https://api.semanticscholar.org/CorpusId:283458176", "relevance": 3, "abstract": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.", "citations": 3}
{"title": "Empirical evidence regarding relations among a model of epistemic and ontological cognition, academic performance, and educational level.", "year": 2010, "authors": "J. A. Greene, J. Torney-Purta, R. Azevedo", "url": "https://www.semanticscholar.org/paper/32c77b346cd58ea7ed0bb1ea82ae1b6b79ecd480", "relevance": 3, "abstract": "", "citations": 112}
{"title": "Can AI Help Us to Understand Belief? Sources, Advances, Limits, and Future Directions", "year": 2021, "authors": "Andrea Vestrucci, S. Lumbreras, L. Oviedo", "url": "https://api.semanticscholar.org/CorpusId:239725912", "relevance": 3, "abstract": "The study of belief is expanding and involves a growing set of disciplines and research areas. These research programs attempt to shed light on the process of believing, understood as a central human cognitive function. Computational systems and, in particular, what we commonly understand as Artificial Intelligence (AI), can provide some insights on how beliefs work as either a linear process or as a complex system. However, the computational approach has undergone some scrutiny, in particular about the differences between what is distinctively human and what can be inferred from AI systems. The present article investigates to what extent recent developments in AI provide new elements to the debate and clarify the process of belief acquisition, consolidation, and recalibration. The article analyses and debates current issues and topics of investigation such as: different models to understand belief, the exploration of belief in an automated reasoning environment, the case of religious beliefs, and future directions of research.", "citations": 9}
{"title": "The Role of the Teacher in Supporting Students\u2019 Epistemic Thinking in Dialogic Argumentation. A Case Study", "year": 2019, "authors": "Roman Svaricek", "url": "https://api.semanticscholar.org/CorpusId:214697320", "relevance": 3, "abstract": "The purpose of this qualitative research paper was to explore the role of a teacher in supporting students' epistemic understanding and argumentation. The main subject of our research was expert teacher Daniela, who had been teaching Czech language arts for twelve years and undertook a developmental program on dialogic teaching three years prior to this study. Data were gathered through structured observations, six video recordings of teaching, and several interviews with the teacher and students. The findings showed that the teacher tried to depersonalise students' arguments and sought to make the argument jointly owned by everybody in the classroom so that it was possible to discuss the nature of the argument and not the student's personal opinion. The findings reveal that the depersonalisation is a unique procedure that could increase students' participation in dialogic argumentation while preserving their personal opinions.", "citations": 3}
{"title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic", "year": 2023, "authors": "Damien Sileo, Antoine Lernould", "url": "https://www.semanticscholar.org/paper/b6ccdd0eb776eee6b317d235e457f20175f380ff", "relevance": 3, "abstract": "Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileod/mindgames", "citations": 32}
{"title": "Analyzing stakeholders\u2019 workshop dialogue for evidence of social learning", "year": 2018, "authors": "A. B. Brymer, J. Wulfhorst, M. Brunson", "url": "https://api.semanticscholar.org/CorpusId:51820506", "relevance": 3, "abstract": "After much debate and synthesis, social learning scholarship is entering an era of empirical research. Given the range across individual-, network-, and systems-level perspectives and scales, clear documentation of social learning processes is critical for making claims about social learning outcomes and their impacts. Past studies have relied on participant recall and concept maps to document perceptions of social learning process and outcome. Using an individual-centric perspective and importing ideas from communication and psychology on question-answer learning through conversational agents, we contribute an expanded conceptual framework and qualitative analytical strategy for assessing stakeholder dialogue for evidence of social learning. We observed stakeholder dialogue across five workshops coordinated for the Bruneau-Owyhee Sage-Grouse Habitat Project (BOSH) in Owyhee County, Idaho, USA. Participants\u2019 dialogue was audio recorded, transcribed, and analyzed for cross-case patterns. Deductive and inductive coding techniques were applied to illuminate cognitive, relational, and epistemic dimensions of learning and topics of learning. A key finding supports our inclusion of the epistemic dimension and highlights a need for future research: although some participants articulated epistemic positions, they did not challenge each other to share sources or justify factual claims. These findings align with previous research suggesting that, in addition to considering diversity and representation (who is at the table), we should pay more attention to how participants talk, perhaps prompting specific patterns of speech as we endeavor to draw causal connections between social learning processes and outcomes.", "citations": 28}
{"title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models", "year": 2023, "authors": "Michal Kosinski", "url": "https://www.semanticscholar.org/paper/464c3a3512d5bde8078185114f38777843d88256", "relevance": 3, "abstract": "", "citations": 244}
{"title": "Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives", "year": 2025, "authors": "Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada", "url": "https://api.semanticscholar.org/CorpusId:282592347", "relevance": 3, "abstract": "Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs'reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs'normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.", "citations": 0}
{"title": "Belief Revision: The Adaptability of Large Language Models Reasoning", "year": 2024, "authors": "Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung", "url": "https://www.semanticscholar.org/paper/64e50707a5398ad0d0cb08cf4b372000f3e14562", "relevance": 3, "abstract": "The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs\u2019 belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning (\\Delta R) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate ~30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs\u2019 adaptiveness to changing information, a step toward more reliable AI systems.", "citations": 9}
{"title": "Epistemic language in news headlines shapes readers\u2019 perceptions of objectivity", "year": 2024, "authors": "Aaron Chuey, Yiwei Luo, E. Markman", "url": "https://www.semanticscholar.org/paper/d2b3e9464ce697ab8cbec52ad095dbf7a254d0e8", "relevance": 3, "abstract": "Significance Headlines are an influential source of information, especially because people often do not read beyond them. We investigated how subtle differences in epistemic language in headlines (e.g., \u201cbelieve\u201d vs. \u201cknow\u201c) affect readers\u2019 inferences about whether claims are perceived as matters of fact or mere opinion. We found, for example, saying \u201cScientists believe methane emissions soared to a record in 2021\u201d led readers to view methane levels as more a matter of opinion compared to saying \u201cScientists know\u2026\u201d Our results provide insight into how epistemic verbs journalists use affect whether claims are perceived as matters of fact and suggest a mechanism contributing to the rise of alternative facts and \u201cpost-truth\u201d politics.", "citations": 7}
{"title": "On the attribution of confidence to large language models", "year": 2024, "authors": "Geoff Keeling, Winnie Street", "url": "https://api.semanticscholar.org/CorpusId:271097704", "relevance": 3, "abstract": "Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis for LLM credence attribution is unclear. We defend three claims. First, our semantic claim is that LLM credence attributions are (at least in general) correctly interpreted literally, as expressing truth-apt beliefs on the part of scientists that purport to describe facts about LLM credences. Second, our metaphysical claim is that the existence of LLM credences is at least plausible, although current evidence is inconclusive. Third, our epistemic claim is that LLM credence attributions made in the empirical literature on LLM evaluation are subject to non-trivial sceptical concerns. It is a distinct possibility that even if LLMs have credences, LLM credence attributions are generally false because the experimental techniques used to assess LLM credences are not truth-tracking.", "citations": 6}
{"title": "Simple Hyperintensional Belief Revision", "year": 2018, "authors": "Francesco Berto", "url": "https://api.semanticscholar.org/CorpusId:126125559", "relevance": 3, "abstract": "I present a possible worlds semantics for a hyperintensional belief revision operator, which reduces the logical idealization of cognitive agents affecting similar operators in doxastic and epistemic logics, as well as in standard AGM belief revision theory. (Revised) belief states are not closed under classical logical consequence; revising by inconsistent information does not perforce lead to trivialization; and revision can be subject to \u2018framing effects\u2019: logically or necessarily equivalent contents can lead to different revisions. Such results are obtained without resorting to non-classical logics, or to non-normal or impossible worlds semantics. The framework combines, instead, a standard semantics for propositional S5 with a simple mereology of contents.", "citations": 42}
{"title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models", "year": 2025, "authors": "Jaeho Lee, Atharv Chowdhary", "url": "https://api.semanticscholar.org/CorpusId:279391960", "relevance": 3, "abstract": "Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to\"stick to its guns\"when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.", "citations": 0}
{"title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering", "year": 2025, "authors": "Eric J. Bigelow, Daniel Wurgaft, YingQiao Wang, Noah D. Goodman, Tomer D. Ullman, Hidenori Tanaka, E. Lubana", "url": "https://www.semanticscholar.org/paper/d3713bac8f1802e9cbe338a3a2ecc646ab78b2d5", "relevance": 3, "abstract": "Large language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.", "citations": 6}
{"title": "BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief", "year": 2021, "authors": "Nora Kassner, Oyvind Tafjord, Hinrich Schutze, Peter Clark", "url": "https://www.semanticscholar.org/paper/ecad7a322ede6de0013b46dc64429eed4c43e8af", "relevance": 3, "abstract": "Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the model actually \u201cbelieves\u201d about the world, making it susceptible to inconsistent behavior and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs \u2013 a BeliefBank \u2013 that records but then may modify the raw PTLM answers. We describe two mechanisms to improve belief consistency in the overall system. First, a reasoning component \u2013 a weighted MaxSAT solver \u2013 revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the accuracy and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.", "citations": 69}
{"title": "Can Bayesian Models of Cognition Show That We Are (Epistemically) Rational?", "year": 2023, "authors": "Arnon Levy", "url": "https://api.semanticscholar.org/CorpusId:257000258", "relevance": 3, "abstract": "Abstract \u201cAccording to [Bayesian] models\u201d in cognitive neuroscience, says a recent textbook, \u201cthe human mind behaves like a capable data scientist.\u201d Do they? That is, do such models show we are rational? I argue that Bayesian models of cognition, perhaps surprisingly, don\u2019t and indeed can\u2019t show that we are Bayes-rational. The key reason is that they appeal to approximations, a fact that carries significant implications. After outlining the argument, I critique two responses, seen in recent cognitive neuroscience. One says that the mind can be seen as approximately Bayes-rational, while the other reconceives norms of rationality.", "citations": 0}
{"title": "Language models cannot reliably distinguish belief from knowledge and fact", "year": 2025, "authors": "Mirac Suzgun, Tayfun Gur, Federico Bianchi, Daniel E. Ho, Thomas Icard, Dan Jurafsky, James Y. Zou", "url": "https://www.semanticscholar.org/paper/6671194eb9913f3276c60a822391834e87216bfd", "relevance": 3, "abstract": "", "citations": 10}
{"title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "year": 2025, "authors": "Quanyan Zhu", "url": "https://www.semanticscholar.org/paper/ba0bdb2accea089e8e5b05a8f1e1ea0d98e777e3", "relevance": 3, "abstract": "We introduce the framework of LLM-Stackelberg games, a class of sequential decision-making models that integrate large language models (LLMs) into strategic interactions between a leader and a follower. Departing from classical Stackelberg assumptions of complete information and rational agents, our formulation allows each agent to reason through structured prompts, generate probabilistic behaviors via LLMs, and adapt their strategies through internal cognition and belief updates. We define two equilibrium concepts: reasoning and behavioral equilibrium, which aligns an agent's internal prompt-based reasoning with observable behavior, and conjectural reasoning equilibrium, which accounts for epistemic uncertainty through parameterized models over an opponent's response. These layered constructs capture bounded rationality, asymmetric information, and meta-cognitive adaptation. We illustrate the framework through a spearphishing case study, where a sender and a recipient engage in a deception game using structured reasoning prompts. This example highlights the cognitive richness and adversarial potential of LLM-mediated interactions. Our results show that LLM-Stackelberg games provide a powerful paradigm for modeling decision-making in domains such as cybersecurity, misinformation, and recommendation systems.", "citations": 2}
{"title": "Vagueness and Aggregation in Multiple Sender Channels", "year": 2017, "authors": "J. Lawry, Oliver James", "url": "https://api.semanticscholar.org/CorpusId:216903832", "relevance": 3, "abstract": "Vagueness is an extremely common feature of natural language, but does it actually play a positive, efficiency enhancing, role in communication? Adopting a probabilistic interpretation of vague terms, we propose that vagueness might act as a source of randomness when deciding what to assert. In this context we investigate the efficacy of multiple sender channels in which senders choose assertions stochastically according to vague definitions of the relevant words, and a receiver then aggregates the different signals. These vague channels are then compared with Boolean channels in which assertions are selected deterministically based on classical (crisp) definitions. We show that given a sufficient number of senders, a linear stochastic channel outperforms Boolean channels when performance is measured by the expected squared error between the actual value described by the senders and the receiver\u2019s estimate of it based on the signals they receive. The number of senders required for vague channels to be at least as accurate as Boolean channels is shown to be a decreasing function of the size of the language i.e. the number of description labels available to the senders. Vague channels are then shown to be robust to transmission error provided the error rate is not too large. In addition, we investigate the behaviour of both Boolean and vague channels for a parametrised family of distributions on the input values. Finally, we consider optimal vague channels assuming a fixed number of senders and show that, provided there are more than two senders, a vague channel can be found that outperforms the optimal Boolean channel. In this context, we show that for channels with relatively low numbers of senders S-curve production functions are optimal.", "citations": 0}
{"title": "Rational quantitative attribution of beliefs, desires and percepts in human mentalizing", "year": 2017, "authors": "Chris L. Baker, J. Jara-Ettinger, R. Saxe, J. Tenenbaum", "url": "https://www.semanticscholar.org/paper/b61cd0fe9d33647cc25f8421f6c8e556b610ed56", "relevance": 1, "abstract": "", "citations": 497}
{"title": "The Role of Epistemic Beliefs in Self-Regulated Learning", "year": 2007, "authors": "Krista R. Muis", "url": "https://www.semanticscholar.org/paper/bae816e93d6acd61176d1ab39f9e767b5de7a7ae", "relevance": 1, "abstract": "", "citations": 416}
{"title": "The case for motivated reasoning.", "year": 1990, "authors": "Ziva Kunda", "url": "https://www.semanticscholar.org/paper/329a0178e56350cf27b41e4cde9c8e278854ec32", "relevance": 1, "abstract": "", "citations": 7788}
{"title": "A Developmental Model of Critical Thinking", "year": 1999, "authors": "D. Kuhn", "url": "https://www.semanticscholar.org/paper/c92f473946d84cf09b8ae3d2fb54c0aa11f793a5", "relevance": 1, "abstract": "", "citations": 1004}
{"title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020, "authors": "Zhengbao Jiang, J. Araki, Haibo Ding, Graham Neubig", "url": "https://www.semanticscholar.org/paper/33422275fbb9958f55419620697faf531482699b", "relevance": 1, "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.", "citations": 521}
{"title": "Calibration of Pre-trained Transformers", "year": 2020, "authors": "Shrey Desai, Greg Durrett", "url": "https://www.semanticscholar.org/paper/52f47e781852a77abedada48cfa971b24c919dde", "relevance": 1, "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.", "citations": 369}
{"title": "Epistemic probabilities are degrees of support, not degrees of (rational) belief", "year": 2023, "authors": "Nevin Climenhaga", "url": "https://api.semanticscholar.org/CorpusId:256483505", "relevance": 1, "abstract": "I argue that when we use \u2018probability\u2019 language in epistemic contexts\u2014e.g., when we ask how probable some hypothesis is, given the evidence available to us\u2014we are talking about degrees of support , rather than degrees of belief . The epistemic probability of A given B is the mind-independent degree to which B supports A, not the degree to which someone with B as their evidence believes A, or the degree to which someone would or should believe A if they had B as their evidence. My central argument is that the degree-of-support interpretation lets us better model good reasoning in certain cases involving old evidence. Degree-of-belief interpretations make the wrong predictions not only about whether old evidence confirms new hypotheses, but about the values of the probabilities that enter into Bayes\u2019 Theorem when we calculate the probability of hypotheses conditional on old evidence and new background information.", "citations": 10}
{"title": "Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings", "year": 2022, "authors": "Neeraj Varshney, Swaroop Mishra, Chitta Baral", "url": "https://www.semanticscholar.org/paper/5248b6e75e7e9e5a96c6cb48e71cbef9b2ea21d3", "relevance": 1, "abstract": "In order to equip NLP systems with \u2018selective prediction\u2019 capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.", "citations": 63}
{"title": "Belief as Willingness to Bet", "year": 2014, "authors": "J. Eijck, B. Renne", "url": "https://api.semanticscholar.org/CorpusId:9402750", "relevance": 1, "abstract": "We investigate modal logics of high probability having two unary modal operators: an operator $K$ expressing probabilistic certainty and an operator $B$ expressing probability exceeding a fixed rational threshold $c\\geq\\frac 12$. Identifying knowledge with the former and belief with the latter, we may think of $c$ as the agent's betting threshold, which leads to the motto \"belief is willingness to bet.\" The logic $\\mathsf{KB.5}$ for $c=\\frac 12$ has an $\\mathsf{S5}$ $K$ modality along with a sub-normal $B$ modality that extends the minimal modal logic $\\mathsf{EMND45}$ by way of four schemes relating $K$ and $B$, one of which is a complex scheme arising out of a theorem due to Scott. Lenzen was the first to use Scott's theorem to show that a version of this logic is sound and complete for the probability interpretation. We reformulate Lenzen's results and present them here in a modern and accessible form. In addition, we introduce a new epistemic neighborhood semantics that will be more familiar to modern modal logicians. Using Scott's theorem, we provide the Lenzen-derivative properties that must be imposed on finite epistemic neighborhood models so as to guarantee the existence of a probability measure respecting the neighborhood function in the appropriate way for threshold $c=\\frac 12$. This yields a link between probabilistic and modal neighborhood semantics that we hope will be of use in future work on modal logics of qualitative probability. We leave open the question of which properties must be imposed on finite epistemic neighborhood models so as to guarantee existence of an appropriate probability measure for thresholds $c\\neq\\frac 12$.", "citations": 12}
